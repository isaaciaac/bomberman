"""Central configuration for the entropy-driven cognitive core demo.

The goal of this module is practical: keep tunable parameters in one place so
experiments (sweeps/ablations) can be configured consistently.
"""

from __future__ import annotations

import json
from dataclasses import dataclass, fields, is_dataclass
from pathlib import Path
from typing import Any, Literal, Mapping

from .entropy import EntropyWeights, HistoryConfig

ValenceMode = Literal["clip_pos", "none", "raw"]
FoldPolicy = Literal["most_similar_pair", "lowest_valence"]
CapacityCostMode = Literal["effective", "total"]
RewriteMode = Literal["search", "template_then_search"]
RewriteEmbedMode = Literal["from_v", "from_q"]
AbstractionEmbedMode = Literal["average", "from_v"]


@dataclass(frozen=True)
class EmbeddingConfig:
    dim: int = 256
    token_min_len: int = 1
    use_english_stopwords: bool = False


@dataclass(frozen=True)
class RetrievalConfig:
    k: int = 5
    valence_mode: ValenceMode = "clip_pos"
    score_threshold: float = 0.0


@dataclass(frozen=True)
class EntropyConfig:
    epsilon: float = 0.35
    weights: EntropyWeights = EntropyWeights()
    coverage_clip_cosine: bool = True
    conflict_include_constraints: bool = False


@dataclass(frozen=True)
class StabilityConfig:
    history_path: Path = Path(".run/history.json")
    history: HistoryConfig = HistoryConfig()
    cluster_tokens: int = 4


@dataclass(frozen=True)
class RewriteConfig:
    tmax: int = 6
    mode: RewriteMode = "search"
    template_path: Path = Path(".run/rewrite_templates.json")
    enabled_kinds: tuple[str, ...] = ("bridge", "constraint", "abstraction")

    # BRIDGE candidate
    bridge_embedding: RewriteEmbedMode = "from_v"
    bridge_cost: float = 0.1
    bridge_valence: float = 0.6

    # CONSTRAINT candidate
    constraint_embedding: RewriteEmbedMode = "from_v"
    constraint_cost: float = 0.05
    constraint_valence: float = 0.3
    constraint_max_pairs: int = 3

    # ABSTRACTION candidate
    abstraction_embedding: AbstractionEmbedMode = "average"
    abstraction_similarity_threshold: float = 0.8
    abstraction_cost_ratio: float = 0.5
    abstraction_valence: float = 0.5


@dataclass(frozen=True)
class WriteBackConfig:
    enabled: bool = False
    stats_path: Path = Path(".run/writeback_stats.json")
    persist_path: Path = Path(".run/memory_store.json")
    min_successes: int = 2
    include_kinds: tuple[str, ...] = ("constraint", "abstraction", "fold")
    max_atoms_per_run: int = 5


@dataclass(frozen=True)
class CapacityConfig:
    enabled: bool = False
    c_max: float = 50.0
    cost_mode: CapacityCostMode = "effective"
    fold_policy: FoldPolicy = "most_similar_pair"
    max_folds_per_write: int = 20
    suppressed_valence: float = 0.0
    fold_cost_ratio: float = 0.5
    fold_valence: float = 0.0
    allow_seed_overlays: bool = True


@dataclass(frozen=True)
class VerifierConfig:
    """External verifier configuration (V(q,M) -> pass/fail).

    The verifier is intentionally deterministic and model-agnostic: it does not
    access the generator and does not change retrieval or rewrites. It is a
    separate acceptance condition for engineering validation.
    """

    enabled: bool = False

    # Reject if any strong conflicts remain among non-constraint atoms.
    require_no_unreconciled_conflicts: bool = True

    # Require at least this many atoms that are not generated by S2
    # (i.e., seed / long-term store items without rewrite_type).
    min_non_generated_atoms: int = 1

    # Require evidence similarity from non-generated atoms.
    #
    # This prevents "synthetic bridge" atoms from trivially lowering E_cov to
    # pass the entropy gate without any real supporting memories.
    min_evidence_sim: float = 0.0
    evidence_clip_cosine: bool = True
    min_evidence_token_overlap: int = 0
    evidence_token_min_len: int = 3
    evidence_use_english_stopwords: bool = True

    # Optional explicit requirements encoded in the query, e.g.:
    #   "... REQ[answer_in_state, other_key]"
    require_claim_keys_from_query: bool = False

    # Optional explicit requirements provided by the task/config (preferred for
    # evaluation, because it keeps queries natural-language).
    #
    # If non-empty, all listed claim keys must be present in M (via either:
    # - parsed FACT/NOT patterns in v_i, or
    # - explicit eta_i.claim_key / eta_i.key).
    required_claim_keys: tuple[str, ...] = ()


@dataclass(frozen=True)
class ModelConfig:
    embedding: EmbeddingConfig = EmbeddingConfig()
    retrieval: RetrievalConfig = RetrievalConfig()
    entropy: EntropyConfig = EntropyConfig()
    stability: StabilityConfig = StabilityConfig()
    rewrite: RewriteConfig = RewriteConfig()
    writeback: WriteBackConfig = WriteBackConfig()
    capacity: CapacityConfig = CapacityConfig()
    verifier: VerifierConfig = VerifierConfig()
    seed_path: Path = Path("data/memory_seed.json")


DEFAULT_CONFIG = ModelConfig()


def _get(d: Mapping[str, Any], key: str, default: Any) -> Any:
    return d[key] if key in d else default


def _as_path(v: Any, default: Path) -> Path:
    if v is None:
        return default
    return Path(str(v))


def model_config_from_dict(raw: Mapping[str, Any]) -> ModelConfig:
    """Build a ModelConfig from a dict, allowing partial overrides."""

    emb = raw.get("embedding", {})
    ret = raw.get("retrieval", {})
    ent = raw.get("entropy", {})
    stab = raw.get("stability", {})
    rw = raw.get("rewrite", {})
    wb = raw.get("writeback", {})
    cap = raw.get("capacity", {})
    ver = raw.get("verifier", {})

    defaults = DEFAULT_CONFIG

    cfg = ModelConfig(
        seed_path=_as_path(raw.get("seed_path"), defaults.seed_path),
        embedding=EmbeddingConfig(
            dim=int(_get(emb, "dim", defaults.embedding.dim)),
            token_min_len=int(_get(emb, "token_min_len", defaults.embedding.token_min_len)),
            use_english_stopwords=bool(_get(emb, "use_english_stopwords", defaults.embedding.use_english_stopwords)),
        ),
        retrieval=RetrievalConfig(
            k=int(_get(ret, "k", defaults.retrieval.k)),
            valence_mode=str(_get(ret, "valence_mode", defaults.retrieval.valence_mode)),  # type: ignore[arg-type]
            score_threshold=float(_get(ret, "score_threshold", defaults.retrieval.score_threshold)),
        ),
        entropy=EntropyConfig(
            epsilon=float(_get(ent, "epsilon", defaults.entropy.epsilon)),
            weights=EntropyWeights(
                alpha=float(_get(ent.get("weights", {}), "alpha", defaults.entropy.weights.alpha)),
                beta=float(_get(ent.get("weights", {}), "beta", defaults.entropy.weights.beta)),
                gamma=float(_get(ent.get("weights", {}), "gamma", defaults.entropy.weights.gamma)),
            ),
            coverage_clip_cosine=bool(_get(ent, "coverage_clip_cosine", defaults.entropy.coverage_clip_cosine)),
            conflict_include_constraints=bool(
                _get(ent, "conflict_include_constraints", defaults.entropy.conflict_include_constraints)
            ),
        ),
        stability=StabilityConfig(
            history_path=_as_path(_get(stab, "history_path", defaults.stability.history_path), defaults.stability.history_path),
            history=HistoryConfig(
                p0=float(_get(stab.get("history", {}), "p0", defaults.stability.history.p0)),
                ema_rate=float(_get(stab.get("history", {}), "ema_rate", defaults.stability.history.ema_rate)),
                include_constraints_in_sig=bool(
                    _get(
                        stab.get("history", {}),
                        "include_constraints_in_sig",
                        defaults.stability.history.include_constraints_in_sig,
                    )
                ),
                include_generated_in_sig=bool(
                    _get(
                        stab.get("history", {}),
                        "include_generated_in_sig",
                        defaults.stability.history.include_generated_in_sig,
                    )
                ),
            ),
            cluster_tokens=int(_get(stab, "cluster_tokens", defaults.stability.cluster_tokens)),
        ),
        rewrite=RewriteConfig(
            tmax=int(_get(rw, "tmax", defaults.rewrite.tmax)),
            mode=str(_get(rw, "mode", defaults.rewrite.mode)),  # type: ignore[arg-type]
            template_path=_as_path(_get(rw, "template_path", defaults.rewrite.template_path), defaults.rewrite.template_path),
            enabled_kinds=tuple(_get(rw, "enabled_kinds", defaults.rewrite.enabled_kinds)),
            bridge_embedding=str(_get(rw, "bridge_embedding", defaults.rewrite.bridge_embedding)),  # type: ignore[arg-type]
            bridge_cost=float(_get(rw, "bridge_cost", defaults.rewrite.bridge_cost)),
            bridge_valence=float(_get(rw, "bridge_valence", defaults.rewrite.bridge_valence)),
            constraint_embedding=str(_get(rw, "constraint_embedding", defaults.rewrite.constraint_embedding)),  # type: ignore[arg-type]
            constraint_cost=float(_get(rw, "constraint_cost", defaults.rewrite.constraint_cost)),
            constraint_valence=float(_get(rw, "constraint_valence", defaults.rewrite.constraint_valence)),
            constraint_max_pairs=int(_get(rw, "constraint_max_pairs", defaults.rewrite.constraint_max_pairs)),
            abstraction_embedding=str(_get(rw, "abstraction_embedding", defaults.rewrite.abstraction_embedding)),  # type: ignore[arg-type]
            abstraction_similarity_threshold=float(
                _get(rw, "abstraction_similarity_threshold", defaults.rewrite.abstraction_similarity_threshold)
            ),
            abstraction_cost_ratio=float(_get(rw, "abstraction_cost_ratio", defaults.rewrite.abstraction_cost_ratio)),
            abstraction_valence=float(_get(rw, "abstraction_valence", defaults.rewrite.abstraction_valence)),
        ),
        writeback=WriteBackConfig(
            enabled=bool(_get(wb, "enabled", defaults.writeback.enabled)),
            stats_path=_as_path(_get(wb, "stats_path", defaults.writeback.stats_path), defaults.writeback.stats_path),
            persist_path=_as_path(_get(wb, "persist_path", defaults.writeback.persist_path), defaults.writeback.persist_path),
            min_successes=int(_get(wb, "min_successes", defaults.writeback.min_successes)),
            include_kinds=tuple(_get(wb, "include_kinds", defaults.writeback.include_kinds)),
            max_atoms_per_run=int(_get(wb, "max_atoms_per_run", defaults.writeback.max_atoms_per_run)),
        ),
        capacity=CapacityConfig(
            enabled=bool(_get(cap, "enabled", defaults.capacity.enabled)),
            c_max=float(_get(cap, "c_max", defaults.capacity.c_max)),
            cost_mode=str(_get(cap, "cost_mode", defaults.capacity.cost_mode)),  # type: ignore[arg-type]
            fold_policy=str(_get(cap, "fold_policy", defaults.capacity.fold_policy)),  # type: ignore[arg-type]
            max_folds_per_write=int(_get(cap, "max_folds_per_write", defaults.capacity.max_folds_per_write)),
            suppressed_valence=float(_get(cap, "suppressed_valence", defaults.capacity.suppressed_valence)),
            fold_cost_ratio=float(_get(cap, "fold_cost_ratio", defaults.capacity.fold_cost_ratio)),
            fold_valence=float(_get(cap, "fold_valence", defaults.capacity.fold_valence)),
            allow_seed_overlays=bool(_get(cap, "allow_seed_overlays", defaults.capacity.allow_seed_overlays)),
        ),
        verifier=VerifierConfig(
            enabled=bool(_get(ver, "enabled", defaults.verifier.enabled)),
            require_no_unreconciled_conflicts=bool(
                _get(
                    ver,
                    "require_no_unreconciled_conflicts",
                    defaults.verifier.require_no_unreconciled_conflicts,
                )
            ),
            min_non_generated_atoms=int(_get(ver, "min_non_generated_atoms", defaults.verifier.min_non_generated_atoms)),
            min_evidence_sim=float(_get(ver, "min_evidence_sim", defaults.verifier.min_evidence_sim)),
            evidence_clip_cosine=bool(_get(ver, "evidence_clip_cosine", defaults.verifier.evidence_clip_cosine)),
            min_evidence_token_overlap=int(
                _get(ver, "min_evidence_token_overlap", defaults.verifier.min_evidence_token_overlap)
            ),
            evidence_token_min_len=int(_get(ver, "evidence_token_min_len", defaults.verifier.evidence_token_min_len)),
            evidence_use_english_stopwords=bool(
                _get(ver, "evidence_use_english_stopwords", defaults.verifier.evidence_use_english_stopwords)
            ),
            require_claim_keys_from_query=bool(
                _get(ver, "require_claim_keys_from_query", defaults.verifier.require_claim_keys_from_query)
            ),
            required_claim_keys=tuple(_get(ver, "required_claim_keys", defaults.verifier.required_claim_keys)),
        ),
    )
    return cfg


def load_model_config(path: Path) -> ModelConfig:
    """Load a ModelConfig from JSON, allowing partial overrides."""

    raw = json.loads(path.read_text(encoding="utf-8"))
    if not isinstance(raw, dict):
        raise ValueError("config JSON must be an object")
    return model_config_from_dict(raw)


def model_config_to_dict(cfg: ModelConfig) -> dict[str, Any]:
    """Convert ModelConfig into a JSON-serializable dict (Paths -> strings)."""

    def convert(obj: Any) -> Any:
        if isinstance(obj, Path):
            return str(obj)
        if is_dataclass(obj):
            return {f.name: convert(getattr(obj, f.name)) for f in fields(obj)}
        if isinstance(obj, dict):
            return {str(k): convert(v) for k, v in obj.items()}
        if isinstance(obj, (list, tuple)):
            return [convert(v) for v in obj]
        return obj

    out = convert(cfg)
    if not isinstance(out, dict):
        raise TypeError("model_config_to_dict expected dict")
    return out


def save_model_config(cfg: ModelConfig, path: Path) -> None:
    """Write a config JSON file for reproducible runs."""

    path.write_text(json.dumps(model_config_to_dict(cfg), indent=2, sort_keys=True), encoding="utf-8")
